{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODq/SJ4ypszBtFoHJdHR5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivster2000/UROPs2025/blob/main/DialoGPT_Controllable_Complexity_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone repo and install dependencies"
      ],
      "metadata": {
        "id": "Uuw0Wyrx3lVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn0Cg8YKaChn",
        "outputId": "c1c42c04-51af-44d8-8287-d21c0dbc78e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ControllableComplexityChatbot'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 55 (delta 21), reused 27 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 1.12 MiB | 5.08 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --single-branch --branch DialoGPT-Adaptation https://github.com/shivster2000/ControllableComplexityChatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ControllableComplexityChatbot/\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Specify the path to the directory you want to add\n",
        "directory_to_add = '/content/ControllableComplexityChatbot/controllable_dialogpt/controllable_dialogpt.py' # Replace with your actual path\n",
        "\n",
        "# Add the directory to sys.path\n",
        "# It's generally good practice to add to the beginning so your modules\n",
        "# are found before others with the same name in standard libraries.\n",
        "if directory_to_add not in sys.path:\n",
        "    sys.path.insert(0, directory_to_add)\n",
        "    print(f\"Added {directory_to_add} to sys.path\")\n",
        "else:\n",
        "    print(f\"{directory_to_add} is already in sys.path\")\n",
        "\n",
        "# You can print sys.path to verify the change\n",
        "# print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etq8KfzxaJkP",
        "outputId": "2ea8ff13-2aca-48cf-c2cd-63f17c52555c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: ControllableComplexityChatbot/: No such file or directory\n",
            "/content/ControllableComplexityChatbot/controllable_dialogpt/controllable_dialogpt.py is already in sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ControllableComplexityChatbot/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n0e7rLpPI65",
        "outputId": "345f8a0e-e99e-4b73-8b8a-5b6c98532287"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'ControllableComplexityChatbot/'\n",
            "/content/ControllableComplexityChatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scipy regex torch transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DpClgx-bTn3",
        "outputId": "78066515-e2bc-441d-8d00-0b3a22213803"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQfd-fJ1cVU_",
        "outputId": "0bf3237a-f740-4653-a9c4-73e60f45dc78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-24 11:09:02.780633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753355343.045218    8851 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753355343.117109    8851 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-24 11:09:03.698703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ControllableComplexityChatbot/demo.py\", line 4, in <module>\n",
            "    from controllable_dialogpt import ControllableDialoGPT\n",
            "  File \"/content/ControllableComplexityChatbot/controllable_dialogpt/__init__.py\", line 1, in <module>\n",
            "    from .controllable_dialogpt import ControllableDialoGPT\n",
            "  File \"/content/ControllableComplexityChatbot/controllable_dialogpt/controllable_dialogpt.py\", line 70, in <module>\n",
            "    print(candidates)\n",
            "          ^^^^^^^^^^\n",
            "NameError: name 'candidates' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo.py"
      ],
      "metadata": {
        "id": "-P0WlIdPcW_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YbrZH7JSFbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "from controllable_dialogpt import ControllableDialoGPT\n",
        "\n",
        "from generation_utils import Reranker, Wordlist, cefr_to_int, load_wordlist\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "allowed_words = load_wordlist(\"data/sample_wordlist.txt\")\n",
        "\n",
        "# Replace agent_opt JSON with dict config\n",
        "config = {\n",
        "    \"model_name\": \"microsoft/DialoGPT-medium\",\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": 0.95,\n",
        "    \"num_return_sequences\": 20,  # for reranking\n",
        "    \"control_method\": \"reranker\",\n",
        "    \"reranker\": Reranker(\n",
        "        model=\"complexity_model\",\n",
        "        tokenizer=\"distilroberta-base\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        cefr=1\n",
        "    )\n",
        "    #\"wordlist\": Wordlist(allowed_words, tokenizer)\n",
        "\n",
        "}\n",
        "\n",
        "agent = ControllableDialoGPT(config)\n",
        "\n",
        "print(\"Start chatting (type 'exit' to stop):\")\n",
        "while True:\n",
        "    user_input = input(\">> User: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    response = agent.generate_response(user_input)\n",
        "    print(f\"ðŸ¤– Bot: {response}\")\n",
        "agent.set_interactive_mode(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPTfFb96ciHs",
        "outputId": "d5a2e8b3-4809-4c1c-d0d8-f11633129a36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] config received: {'model_name': 'microsoft/DialoGPT-medium', 'device': 'cpu', 'top_k': 40, 'top_p': 0.95, 'num_return_sequences': 20, 'control_method': 'reranker', 'reranker': <generation_utils.Reranker object at 0x7f5fb143f410>}\n",
            "[DEBUG] reranker set: <generation_utils.Reranker object at 0x7f5fb143f410>\n",
            "[DEBUG] wordlist set: None\n",
            "[DEBUG] init config keys: dict_keys(['model_name', 'device', 'top_k', 'top_p', 'num_return_sequences', 'control_method', 'reranker'])\n",
            "[DEBUG] reranker in config: True\n",
            "[DEBUG] wordlist in config: False\n",
            "[DEBUG] control_method resolved: rerank\n",
            "Start chatting (type 'exit' to stop):\n",
            ">> User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#controllable_dialogpt.py"
      ],
      "metadata": {
        "id": "6B_IL1S5z0Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python controllable_dialogpt/controllable_dialogpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89cmu0x6B-6n",
        "outputId": "056a9860-e309-40d2-8e20-27bab69c5722"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-24 12:32:21.501265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753360341.783604   30076 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753360341.864838   30076 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
        "\n",
        "from generation_utils import Wordlist, Reranker, cefr_to_int, load_wordlist\n",
        "\n",
        "class ControllableDialoGPT:\n",
        "    def __init__(self, config):\n",
        "        print(\"[DEBUG] config received:\", config)\n",
        "        self.config = config\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.history = None\n",
        "        self.reranker = config.get(\"reranker\", None)\n",
        "        print(\"[DEBUG] reranker instance:\", self.reranker)\n",
        "        self.wordlist = config.get(\"wordlist\", None)\n",
        "\n",
        "        print(\"[DEBUG] reranker set:\", self.reranker)\n",
        "        print(\"[DEBUG] wordlist set:\", self.wordlist)\n",
        "\n",
        "        self.interactive_mode = config.get(\"interactive_mode\", False)\n",
        "\n",
        "        print(\"[DEBUG] init config keys:\", config.keys())\n",
        "        print(\"[DEBUG] reranker in config:\", \"reranker\" in config)\n",
        "        print(\"[DEBUG] wordlist in config:\", \"wordlist\" in config)\n",
        "\n",
        "        if self.reranker and self.wordlist:\n",
        "            raise ValueError(\"Choose only one control method: reranker or wordlist.\")\n",
        "\n",
        "        self.control_method = \"reranker\" if self.reranker else \"wordlist\" if self.wordlist else \"none\"\n",
        "\n",
        "        print(\"[DEBUG] control_method resolved:\", self.control_method)\n",
        "\n",
        "    def set_interactive_mode(self, mode: bool):\n",
        "        self.interactive_mode = mode\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        input_ids = self.tokenizer.encode(user_input + self.tokenizer.eos_token, return_tensors=\"pt\").to(self.device)\n",
        "        full_input = torch.cat([self.history, input_ids], dim=-1) if self.history is not None else input_ids\n",
        "\n",
        "        attention_mask = torch.ones(full_input.shape, device=self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            full_input,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=1000,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            top_k=40,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=20 if self.reranker else 1\n",
        "        )\n",
        "\n",
        "\n",
        "        candidates = [\n",
        "            self.tokenizer.decode(out[full_input.shape[-1]:], skip_special_tokens=True)\n",
        "            for out in outputs\n",
        "        ]\n",
        "\n",
        "        if self.control_method == \"wordlist\":\n",
        "            candidates = [c for c in candidates if self._passes_vocab_filter(c)]\n",
        "            print(f\"candidates after wordlist filter: {candidates}\")\n",
        "            if not candidates:\n",
        "                return \"[No cheesy response]\"\n",
        "\n",
        "        if self.control_method == \"reranker\":\n",
        "            print(\"[DEBUG] Using reranker for selection\")\n",
        "            best_response = self.reranker.rank(self, candidates)\n",
        "\n",
        "        else:\n",
        "            best_response = candidates[0]\n",
        "\n",
        "        self.history = torch.cat([full_input, self.tokenizer.encode(best_response + self.tokenizer.eos_token, return_tensors=\"pt\").to(self.device)], dim=-1)\n",
        "\n",
        "        return best_response\n",
        "\n",
        "    def _passes_vocab_filter(self, text: str) -> bool:\n",
        "\n",
        "        tokens = text.strip().split()\n",
        "        return all(word.lower().strip(\".,!?\") in self.wordlist.allowed_token_seqs for word in tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "pAtWt8Il9_oJ",
        "outputId": "a0f10bc8-e184-411f-cf93-2a6b85ab1c87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'generation_utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3876415317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeneration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReranker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mControllableDialoGPT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'generation_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generation_utils.py - move to main folder"
      ],
      "metadata": {
        "id": "KVHlKO_gO3IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import json\n",
        "from typing import List, Set, Dict, Union, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "def cefr_to_int(cefr: str) -> int:\n",
        "    mapping = {\n",
        "        \"A1\": 0,\n",
        "        \"A2\": 1,\n",
        "        \"B1\": 2,\n",
        "        \"B2\": 3,\n",
        "        \"C1\": 4,\n",
        "        \"C2\": 5,\n",
        "    }\n",
        "    clean_cefr = cefr.upper().strip()\n",
        "    assert clean_cefr in mapping, f\"CEFR must be one of {list(mapping.keys())}, not {cefr}\"\n",
        "\n",
        "    return mapping[clean_cefr]\n",
        "\n",
        "\n",
        "def load_wordlist(path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load a list of words from a text file containing one word per line\n",
        "    \"\"\"\n",
        "    vocab = []\n",
        "\n",
        "    if not path:\n",
        "        return vocab\n",
        "\n",
        "    assert os.path.isfile(path)\n",
        "\n",
        "    with open(path, 'r', encoding=\"utf-8\") as vocab_file:\n",
        "        for row in vocab_file:\n",
        "            token = row.strip()\n",
        "            vocab.append(token)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "class Wordlist():\n",
        "    def __init__(self, allowed_words: List[str], tokenizer: PreTrainedTokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Identify token ID sequences that are allowed words\n",
        "        # Identify allowed continuations of sequences\n",
        "        self.allowed_token_seqs: Set[str] = set()\n",
        "        self.allowed_continuations: Dict[str, List[int]] = {}\n",
        "\n",
        "        for word in allowed_words:\n",
        "            for word_variant in self._get_word_variants(word):\n",
        "                token_ids = tokenizer.encode(word_variant, add_special_tokens=False)\n",
        "                if not token_ids:\n",
        "                    continue\n",
        "\n",
        "                self.allowed_token_seqs.add(repr(token_ids))\n",
        "\n",
        "                for i in range(1, len(token_ids)):\n",
        "                    prefix = repr(token_ids[:i])\n",
        "                    next_token = token_ids[i]    # List represented as string for lookup\n",
        "                    if prefix not in self.allowed_continuations:\n",
        "                        self.allowed_continuations[prefix] = []\n",
        "                    self.allowed_continuations[prefix].append(next_token)\n",
        "\n",
        "    def get_allowed_ids(self, token_ids: List[int]) -> List[int]:\n",
        "        \"\"\"\n",
        "        adapts parlai-based _get_continuation_ids function\n",
        "        \"\"\"\n",
        "        last_word = self._get_last_word(token_ids)\n",
        "        prefix_str = repr(last_word)\n",
        "        continuation_ids = self.allowed_continuations.get(prefix_str, [])\n",
        "\n",
        "        if self._is_word(last_word) or not last_word:\n",
        "          continuation_ids += self._get_starting_tokens()\n",
        "\n",
        "        return list(set(continuation_ids))\n",
        "\n",
        "\n",
        "    def _is_word(self, token_ids: List[int]) -> bool:\n",
        "        \"\"\"\n",
        "        For a given sequence of token IDs, determine whether that sequence is a complete word\n",
        "        \"\"\"\n",
        "        return repr(token_ids) in self.allowed_token_seqs\n",
        "\n",
        "\n",
        "    def _get_last_word(self, token_ids: List[int]) -> List[int]:\n",
        "        \"\"\"\n",
        "        Get the sequence of token IDs after the last word boundary.\n",
        "        Assumes that a word boundary is denoted by punctuation or whitespace (Ä ).\n",
        "        \"\"\"\n",
        "\n",
        "        if not token_ids:\n",
        "          return []\n",
        "\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        for i in range(len(token_ids) -1, -1, -1):\n",
        "            if tokens[i].startswith(\"Ä \") or tokens[i].startswith(\"_\"):\n",
        "                return token_ids[i:]\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def _get_starting_tokens(self) -> List[int]:\n",
        "        \"\"\"\n",
        "        return token IDs that can start a word (e.g. Ä  or_ initial)\n",
        "        \"\"\"\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "        starting_tokens = [idx for tok, idx in vocab.items() if tok.startswith(\"Ä \") or tok.startswith(\"_\") or tok.isalpha()]\n",
        "        return starting_tokens\n",
        "\n",
        "\n",
        "    def _get_word_variants(self, word: str) -> Set[str]:\n",
        "        return {word, word.lower(), word.capitalize()}\n",
        "\n",
        "\n",
        "\n",
        "class Reranker():\n",
        "    def __init__(self,\n",
        "                 cefr: int,\n",
        "                 model: str,\n",
        "                 tokenizer: str = \"distilroberta-base\",\n",
        "                 device: Optional[str] = \"cuda\",\n",
        "                 text_truncate: int = 128,\n",
        "                 exempt_tokens: Union[str, List[int]] = \"all\",\n",
        "                 penalty_stddev: int = 2,\n",
        "                 vocab_size: int = 8008,\n",
        "                 word_filter: Optional[List[str]] = None):\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer)\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(model)\n",
        "        self.model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        self.target_cefr = cefr\n",
        "        self.text_truncate = text_truncate\n",
        "        self.word_filter = word_filter\n",
        "\n",
        "        cefr_filepath = os.path.join(os.path.dirname(__file__), 'tokens_by_cefr.json')\n",
        "        with open(cefr_filepath, 'r') as cefr_file:\n",
        "            token_cefrs = json.load(cefr_file)\n",
        "\n",
        "        if exempt_tokens == \"all\" or penalty_stddev < 0:      # No penalties\n",
        "            self.token_penalties = torch.tensor([[1] * vocab_size])\n",
        "        else:\n",
        "            # calculate penalties per CEFR level difference (0 = same CEFR)\n",
        "            normal_dist = torch.distributions.normal.Normal(0, penalty_stddev)\n",
        "            cefr_penalties = [math.exp(normal_dist.log_prob(torch.tensor(i))) for i in range(6)]\n",
        "\n",
        "            token_penalties = []\n",
        "            for i in range(vocab_size):\n",
        "                if i in exempt_tokens:\n",
        "                    token_penalties.append(cefr_penalties[0])\n",
        "\n",
        "                elif str(i) in token_cefrs:\n",
        "                    token_str, token_cefr = token_cefrs[str(i)]\n",
        "                    penalty = cefr_penalties[int(token_cefr - self.target_cefr)]\n",
        "\n",
        "                    if token_cefr <= self.target_cefr or not token_str.isalpha():         # ignore lower CEFR levels and punctuation/special tokens\n",
        "                        penalty = cefr_penalties[0]\n",
        "\n",
        "                    token_penalties.append(penalty)\n",
        "\n",
        "                else:       # Assume highest CEFR level if we don't have an assigned CEFR level\n",
        "                    token_penalties.append(cefr_penalties[int(5 - self.target_cefr)])\n",
        "\n",
        "            self.token_penalties = torch.tensor([token_penalties])\n",
        "\n",
        "    def get_complexity_scores(self, hyps: List[str]) -> np.ndarray:\n",
        "        model_inputs = self.tokenizer(hyps,\n",
        "                                      padding='max_length',\n",
        "                                      truncation=True,\n",
        "                                      max_length=self.text_truncate,\n",
        "                                      return_tensors='pt',\n",
        "                                      return_token_type_ids=True,\n",
        "                                      return_attention_mask=True)\n",
        "\n",
        "        model_output = self.model(input_ids=model_inputs[\"input_ids\"].to(self.device),\n",
        "                                  attention_mask=model_inputs[\"attention_mask\"].to(self.device),\n",
        "                                  token_type_ids=model_inputs[\"token_type_ids\"].to(self.device))\n",
        "\n",
        "        complexity_scores = model_output.logits.cpu().numpy().flatten()\n",
        "        complexity_diffs = 5 - np.absolute(complexity_scores - self.target_cefr)      # reversed so that higher score = better\n",
        "\n",
        "        return complexity_diffs\n",
        "\n",
        "    def rank(self, candidates: List[str]) -> str:\n",
        "        print(\"[DEBUG] rank() called with\", len(candidates), \"candidates\")\n",
        "        scores = self.get_complexity_scores(candidates)\n",
        "        print(\"\\n[DEBUG] Reranker scores:\")\n",
        "        for i, (text, score) in enumerate(zip(candidates, scores)):\n",
        "            print(f\"Candidate {i}: Score={score:.2f} -> {text}\")\n",
        "\n",
        "        best_index = scores.argmax()\n",
        "        return candidates[best_index]\n",
        "\n"
      ],
      "metadata": {
        "id": "A-A9ABStO_BO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _init.py"
      ],
      "metadata": {
        "id": "aQH_W73iHr6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from .controllable_dialogpt import ControllableDialoGPT\n"
      ],
      "metadata": {
        "id": "0xt2Umg9Hzer"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}